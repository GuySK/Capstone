savehistory("~/.Rhistory")
setwd("C:/Users/AAB330/Google Drive 2/Training/DataScience/CapstonePj")
load("C:/Users/AAB330/Google Drive 2/Training/DataScience/CapstonePj/env.RData")
# set up environment variable for wordnet
Sys.setenv(WNHOME="C:/Program Files (x86)/WordNet/2.1")
library(wordnet)
if (!initDict())
setDict(paste0(Sys.getenv("WNHOME"),"/dict"))
library(rJava)
library(tm)
setwd("C:/Users/AAB330/Google Drive 2/Training/DataScience/CapstonePj/AWS")
source('C:/Users/AAB330/Google Drive 2/Training/DataScience/CapstonePj/AWS/eda1_auxFunctions.R', echo=TRUE)
is.english('washed')
is.english('washed')
word = 'flower'
is.english <- function(words) {
# Checks if words in wordnet
# lookup function
lookup <- function(word) {
# looks up a word in wordnet
pos <- c("ADJECTIVE", "ADVERB", "NOUN", "VERB")
term <- NULL
lemma <- NULL
idx <- 1
while (is.null(term)){
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
term <- getIndexTerms(pos[idx], 25, filter)
idx <- idx + 1
if (idx > length(pos))
break
}
return(term)
}
}
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
term <- getIndexTerms('ADJECTIVE', 25, filter)
term
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
term <- getIndexTerms('ADVERB', 25, filter)
term
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
term <- getIndexTerms('NOUN', 25, filter)
term
class(term)
term
str(term)
term[[1]]
class(term[[1]])
term <- unlist(term)
term
class(term)
term <- unlist(term)
term
term[[1]]$getLemma()
term[[1]]$getPartOfSpeech()
term[[1]]$getLemma()
term[[1]]$getSynsets()
term[[1]]$getPartOfSpeech()
pos <- term[[1]]$getPartOfSpeech()
pos
pos$toString()
?getTermFilterr
?getTermFilter
??wordnet
term$getPartofSpeech()
pos <- term[[1]]$getPartOfSpeech()
pos
pos <- term[[1]]$getPartOfSpeech()$toString
pos
pos <- term[[1]]$getPartOfSpeech()
pos
pos$toString()
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
term <- getIndexTerms('NOUN', 25, filter)
word = 'jj'
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
term <- getIndexTerms('NOUN', 25, filter)
term
term == NULL
is.null(term)
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
term <- getIndexTerms('NOUN', 25, filter)
word = 'flower'
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
term <- getIndexTerms('NOUN', 25, filter)
term
term[[1]]$getPartOfSpeech()
term[[1]]$getPartOfSpeech()$toString()
#
#
#
getPOS <- function(word) {
# Looks up word in Wordnet.
# Returns char vector with all word's POS.
pos <- c("ADJECTIVE", "ADVERB", "NOUN", "VERB")
POS <- c()
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
for (i in 1:length(pos)){
term <- getIndexTerms(pos[idx], 25, filter)
if (!(is.null(term)))
POS <- append(POS, pos[i])
}
return(POS)
}
getPOS('flower')
#
#
#
getPOS <- function(word) {
# Looks up word in Wordnet.
# Returns char vector with all word's POS.
pos <- c("ADJECTIVE", "ADVERB", "NOUN", "VERB")
POS <- c()
filter <- getTermFilter("ExactMatchFilter", word, TRUE)
for (i in 1:length(pos)){
term <- getIndexTerms(pos[i], 25, filter)
if (!(is.null(term)))
POS <- append(POS, pos[i])
}
return(POS)
}
getPOS('flower')
getPOS('John')
getPOS('FlowER')
getPOS('get')
getPOS('google')
getPOS('will')
getPOS('yelow')
getPOS('yellow')
is.english <- function(x){
return(!is.null(getPOS(x)))
}
is.english('google')
is.english('pit')
is.english('vaca')
is.english('loco')
is.english('cabeza')
is.english('bandido')
is.english('bandit')
is.english(c('bandit', 'pit'))
sapply(X = c('bandit', 'pit'), FUN = is.english)
trData[1]
length(trData[1])
ngram(trData[1],1)
sapply(X = ngram(trData[1], 1), FUN = is.english)
sapply(X = ngram(trData[1], 1), FUN = getPOS)
pos <- sapply(X = ngram(trData[1], 1), FUN = getPOS)
pos[[1]]
names(pos)
ngram(trData[1],1)
source('~/.active-rstudio-document', echo=TRUE)
getPOS('flower')
paste(c('a', 'b'), collapse='')
source('~/.active-rstudio-document', echo=TRUE)
getPOS('flower')
getPOS('slat')
getPOS('slut')
getPOS('sder')
getPOS('cava')
a <- c()
is.null(a)
source('C:/Users/AAB330/Google Drive 2/Training/DataScience/CapstonePj/AWS/wordNet.R', echo=TRUE)
getPOS('cava')
getPOS('cat')
pos <- sapply(X = ngram(trData[1], 1), FUN = getPOS)
pos
ngram(trData[1], 2)
p <- paste(c('a', 'b'), collapse='')
class(p)
length(p)
p
sapply(trData[1], is.english)
sapply(ngram(trData[1],1), is.english)
sapply(ngram(trData[1],1), getPOS)
unlist(sapply(ngram(trData[1],1), getPOS))
# it's slow for tagging
# example
nyt = "As another round of fiscal brinkmanship looms with Republican control of Congress, the impact on tax policy, government programs and the overall economy could be severe.
"
unlist(sapply(ngram(nyt,1), getPOS))
ngram(nyt, 1)
# it's slow for tagging
# example
nyt = "As another round of fiscal brinkmanship looms with Republican control of Congress, the impact on tax policy, government programs and the overall economy could be severe.
"
strt <- Sys.time()
unlist(sapply(ngram(nyt,1), getPOS))
(elapsed <- Sys.time() - strt)
#
?paste0
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
ngramPos(nyy, 1)
ngramPos(nyt, 1)
posTag <- function(word){
return(paste0(word, '/', getPOS(word)))
}
posTag('flower')
ngram(nyt,1)
posTag(nyt[1:2])
# POS Tagger function
posTag <- function(words){
tagged <- c()
for (i in 1:length(words)){
tagged <- append(tagged, paste0(word, '/', getPOS(word)))
}
return(tagged)
}
posTag(nyt[1:2])
# POS Tagger function
posTag <- function(words){
tagged <- c()
for (i in 1:length(words)){
tagged <- append(tagged, paste0(words[i], '/', getPOS(words[i])))
}
return(tagged)
}
posTag(nyt[1:2])
posTag(ngram(nyt,1))
strt <- Sys.time()
posTag(ngram(nyt,1))
(elapsed <- Sys.time() - strt)
return(unlist(ngrams))
ngramPos <- function(x, n=2, split=" ", sep="::",
startMark='<s>', stopMark='</s>'){
# Takes a vector of strings, size of n-grams to generate, split char and separator
# Returns a vector of n-grams
# recursive ngram generator
ngrm <- function(words, n, sep, ngrams){
if (length(words) < n)                    # no more n-grams, end of story
return(unlist(ngrams))
ngrams <- append(ngrams, list(paste(posTag(words[1:n]), collapse=sep)))
return(ngrm(words[2:(length(words))], n, sep, ngrams))
}
# wrapper function
if(split == " ")                              # if split by whitespaces
split <- "\\s"                            # use it as split char but
x <- gsub(paste0(split,"+"), " ", x)          # make sure only one between wrds
x <- gsub("^\\s+", "", x)                     # and none as first character
words <- unlist(strsplit(x,split = split))    # create vector of words
if (n > 1) {
words <- append(startMark, words)         # add start...
words <- append(words, stopMark)          # and stop markers
}
ngrams <- list()                              # list of ngrams
if (n < 2)                                    # just return input words
return(posTag(words))                     # or empty vector
return(ngrm(words, n, sep, ngrams))           # not a trivial case. call gen.
}
ngramPos(nyt, 1)
strt <- Sys.time()
ngramPos(nyt, 1)
(elapsed <- Sys.time() - strt)
strt <- Sys.time()
posTag(ngram(nyt,1))
(elapsed <- Sys.time() - strt)
strt <- Sys.time()
unlist(sapply(ngram(nyt,1), getPOS))
(elapsed <- Sys.time() - strt)
words <- ngram(nyt,1)
strt <- Sys.time()
unlist(sapply(words, getPOS))
(elapsed <- Sys.time() - strt)
words
strt <- Sys.time()
posTag(words)
(elapsed <- Sys.time() - strt)
#
trData[1]
trData[2]
nyt
posTag(trData[2])
posTag(ngram(trData[2],1))
posTag(ngram(nyt,1))
is.english('of')
nyt
install.packages("koRpus")
library("koRpus", lib.loc="~/R/win-library/3.0")
detach("package:koRpus", unload=TRUE)
tokenize(nyt)
?tokenize
??tokenize
library("koRpus", lib.loc="~/R/win-library/3.0")
tokenize(nyt)
tokenize(nyt, format='obj')
tokenize(nyt, format='obj', lang='en')
treetag(nyt)
save.image("C:/Users/AAB330/Google Drive 2/Training/DataScience/CapstonePj/env.RData")
savehistory("C:/Users/AAB330/Google Drive 2/Training/DataScience/CapstonePj/.Rhistory")
