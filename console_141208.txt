> source('C:/Users/AAB330/Google Drive 2/Training/DataScience/CapstonePj/AWS/modelling_8.R', echo=TRUE)

> #'
> #'
> #' Capstone Project
> #' 
> #' Task 3 - Modeling - 8: blogs data set
> #' Model based on n-grams. 
> #'
> #'
> # ---
> 
> # --------       .... [TRUNCATED] 

> DSET = 1;                             # Blogs=1, News = 2, Tweets=3

> TRAINPROP = 0.05;                     # Proportion of data set to be used as training set

> SEED_N = 191;                         # Seed for random operations

> nCode_STEPS = 10;                     # Number of steps to encode training data

> REC_LIMIT = 1;                        # Proportion of records to process in this run

> EXPRS_N = 100000;                     # limit on number of nested expr to be evaluated

> ngram_THRESH = 2;                     # Min frequency of ngrams to keep

> CHUNK_PROP = 0.1;                     # Default chunk size for chopping function

> UNK_LMT = 0.005;                      # quantile limit to replace for <UNK>

> # --------                            ------------------------------------------------
> 
> cat('>>> Script settings. \n')
>>> Script settings. 

> cat('    AWS =', AWS, '\n')
    AWS = FALSE 

> cat('    DSET =', DSET, '\n')
    DSET = 1 

> cat('    TRAINPROP =', TRAINPROP, '\n')
    TRAINPROP = 0.05 

> cat('    SEED_N =', SEED_N, '\n')
    SEED_N = 191 

> cat('    nCode_STEPS =', nCode_STEPS, '\n')
    nCode_STEPS = 10 

> cat('    REC_LIMIT =', REC_LIMIT, '\n')
    REC_LIMIT = 1 

> cat('    EXPRS_N =', EXPRS_N, '\n')
    EXPRS_N = 1e+05 

> cat('    ngram_THRESH =', ngram_THRESH, '\n')
    ngram_THRESH = 2 

> cat('    CHUNK_PROP =', CHUNK_PROP, '\n')
    CHUNK_PROP = 0.1 

> # ---
> cat(">>> Basic setup. \n")
>>> Basic setup. 

> if (AWS) {
+     setwd("~/gsk1")
+ } else {
+     setwd("~/../Google Drive 2/Training/DataScience/CapstonePj")
+ }

> source("./AWS/eda1_auxFunctions.R")

> source("./AWS/AuxFunctions.R")

> source("./AWS/trnsFuncts.R")

> source("./AWS/Ngram_tf.R")

> library(tm)

> library(hash)

> # --- increase options setting for recursive functions
> options(expressions=EXPRS_N)

> # ---
> sourceDir = './DataSets'

> LANG = 'eng'

> cat(">>> Loading corpus. \n")
>>> Loading corpus. 

> pause("Continue????")
Continue???? 
Press 'C/c' to cancel script execution or 'Enter' to continue.

Script execution continues. 

> # Script run continues 
> 
> if (file.access('Corpus_en', 4) == 0) {
+   cat('>>> Loading object from disk. \n')
+   load('Corpus_en')
+ } else {
+  .... [TRUNCATED] 
>>> Loading object from disk. 

> # --- Getting data set and training data
> cat('>>> Getting data set and training data. \n')
>>> Getting data set and training data. 

> dset <- crp[[DSET]]

> if (AWS)                                    
+     dset <- unlist(dset[1]) 

> if (file.access('Corpus_en', 0) == -1)
+   save(crp, file='Corpus_en')

> rm(crp); gc()
           used  (Mb) gc trigger  (Mb)  max used   (Mb)
Ncells  1521188  81.3    4953636 264.6   4953636  264.6
Vcells 30861961 235.5   99010294 755.4 158358480 1208.2

> # --- Creating Vocabulary
> cat('>>> Creating Vocabulary. \n')
>>> Creating Vocabulary. 

> pause('Proceed?')
Proceed? 
Press 'C/c' to cancel script execution or 'Enter' to continue.

Script execution continues. 

> #
> set.seed(SEED_N)

> train <- sample(1:length(dset), length(dset) * TRAINPROP)

> strt <- Sys.time()

> ctrlList <- list(convertTolower=c(TRUE, 3),
+                  verbose=TRUE,
+                  convertToASCII=TRUE,
+                  removePunct= .... [TRUNCATED] 

> source('createVocab.R')
>>> cleaning document. Control options are: 
--- convertTolower 1 3 
--- verbose TRUE 
--- convertToASCII TRUE 
--- removePunct TRUE 
--- removeNumbers TRUE 
--- removeStopWords FALSE 
>>> converting to lowercase in steps. 
>>> Processing object in 3 steps. 
>>> Step 1.
>>> 14988 records processed in this step. 
>>> Step 2.
>>> 14988 records processed in this step. 
>>> Step 3.
>>> 14988 records processed in this step. 
>>> removing numbers. 
>>> removing punctuation  
>>> getting rid of strange characters. 
>>> Applying function to a text with 44964 records. 
............................................EoF 
>>> End of Job. 
>>> Creating 1-grams 
    1846983 terms generated. 
>>> Estimating time for main process. 
    Estimated time for main process:  1.86 mins 
>>> Partitioning input file in 10 parts of size 184698 
>>> Terms integrity checked OK 
>>> Creating term frequencies dicts. 
    Processing list 1 with 193781 terms. 
    Processing list 2 with 183862 terms. 
    Processing list 3 with 183682 terms. 
    Processing list 4 with 201401 terms. 
    Processing list 5 with 181000 terms. 
    Processing list 6 with 181579 terms. 
    Processing list 7 with 180419 terms. 
    Processing list 8 with 209935 terms. 
    Processing list 9 with 165872 terms. 
    Processing list 10 with 165452 terms. 
>>> Clearing memory and unlisting dicts. 
>>> End of Job. Job time was: 1.93 mins 
>>> Replacing unfrequent terms as unknown. 
>>> Creating hash table. 
>>> End of script --createVocab--. 

> #
> (elapsed <- Sys.time() - strt)
Time difference of 2.529278 mins

> # --- Encode training data
> cat('>>> Encoding training data. \n')
>>> Encoding training data. 

> pause('Proceed?')
Proceed? 
Press 'C/c' to cancel script execution or 'Enter' to continue.

Script execution continues. 

> # Script continues.
> 
> # Encoding data in steps 
> recLimit = as.integer(length(trData) * REC_LIMIT)  

> strt <- Sys.time()  

> stepLst <- step(x=trData[1:recLimit], fun=lapply, steps=nCode_STEPS, nCode)
>>> Processing object in 10 steps. 
>>> Step 1.
>>> 4496 records processed in this step. 
>>> Step 2.
>>> 4496 records processed in this step. 
>>> Step 3.
>>> 4496 records processed in this step. 
>>> Step 4.
>>> 4496 records processed in this step. 
>>> Step 5.
>>> 4496 records processed in this step. 
>>> Step 6.
>>> 4496 records processed in this step. 
>>> Step 7.
>>> 4496 records processed in this step. 
>>> Step 8.
>>> 4496 records processed in this step. 
>>> Step 9.
>>> 4496 records processed in this step. 
>>> Step 10.
>>> 4500 records processed in this step. 

> trData_n <- unlist(stepLst, recursive=F, use.names=F)

> (elapsed <- Sys.time() - strt)
Time difference of 24.51882 mins

> # --- bigrams
> cat('>>> Creating bigrams. \n')
>>> Creating bigrams. 

> ngramLst <- Ngram.tf(x = trData_n, n = 2, fun = ngramN, 
+                      threshold = ngram_THRESH, 
+                      chunkSize = CHUNK_ .... [TRUNCATED] 
>>> Creating 2-grams 
    1891947 terms generated. 
>>> Estimating time for main process. 
    Estimated time for main process:  12.26 mins 
>>> Partitioning input file in 10 parts of size 189194 
>>> Terms integrity checked OK 
>>> Creating term frequencies dicts. 
    Processing list 1 with 189194 terms. 
    Processing list 2 with 189331 terms. 
    Processing list 3 with 189180 terms. 
    Processing list 4 with 189188 terms. 
    Processing list 5 with 195128 terms. 
    Processing list 6 with 188898 terms. 
    Processing list 7 with 187823 terms. 
    Processing list 8 with 188054 terms. 
    Processing list 9 with 187684 terms. 
    Processing list 10 with 187467 terms. 
>>> Clearing memory and unlisting dicts. 
>>> End of Job. Job time was: 8.69 mins 

> #
> n2g <- ngramLst[[1]]

> n2gp <- ngramLst[[2]]

> T2 <- length(n2g)        # Number of Types

> N2 <- sum(n2g)           # Number of tokens

> n2g_df <- ngram.DF(n2g)                   

> # --- trigrams
> cat('>>> Creating trigrams. \n')
>>> Creating trigrams. 

> ngramLst <- Ngram.tf(x = trData_n, n = 3, fun = ngramN, 
+                      threshold = ngram_THRESH, 
+                      chunkSize = CHUNK_ .... [TRUNCATED] 
>>> Creating 3-grams 
    1846983 terms generated. 
>>> Estimating time for main process. 
    Estimated time for main process:  31.91 mins 
>>> Partitioning input file in 10 parts of size 184698 
>>> Terms integrity checked OK 
>>> Creating term frequencies dicts. 
    Processing list 1 with 184698 terms. 
    Processing list 2 with 184698 terms. 
    Processing list 3 with 184698 terms. 
    Processing list 4 with 184736 terms. 
    Processing list 5 with 184695 terms. 
    Processing list 6 with 184691 terms. 
    Processing list 7 with 184691 terms. 
    Processing list 8 with 184692 terms. 
    Processing list 9 with 184692 terms. 
    Processing list 10 with 184692 terms. 
>>> Clearing memory and unlisting dicts. 
>>> End of Job. Job time was: 41.34 mins 

> #
> n3g <- ngramLst[[1]]

> n3gp <- ngramLst[[2]]

> T3 <- length(n3g)    # Number of Types

> N3 <- sum(n3g)       # Number of tokens

> n3g_df <- ngram.DF(n3g)

> # --- skip 2-grams
> cat('>>> Creating skip bigrams. \n')
>>> Creating skip bigrams. 

> ctrlList <- list(convertTolower=c(TRUE, 3),
+                  verbose=TRUE,
+                  convertToASCII=TRUE,
+                  removePunct= .... [TRUNCATED] 

> cat('>>> Recreating vocab without stop words. \n')
>>> Recreating vocab without stop words. 

> source('createVocab.R')
>>> cleaning document. Control options are: 
--- convertTolower 1 3 
--- verbose TRUE 
--- convertToASCII TRUE 
--- removePunct TRUE 
--- removeNumbers TRUE 
--- removeStopWords TRUE en 
>>> converting to lowercase in steps. 
>>> Processing object in 3 steps. 
>>> Step 1.
>>> 14988 records processed in this step. 
>>> Step 2.
>>> 14988 records processed in this step. 
>>> Step 3.
>>> 14988 records processed in this step. 
>>> removing stop words. 
>>> removing numbers. 
>>> removing punctuation  
>>> getting rid of strange characters. 
>>> Applying function to a text with 44964 records. 
............................................EoF 
>>> End of Job. 
>>> Creating 1-grams 
    968910 terms generated. 
>>> Estimating time for main process. 
    Estimated time for main process:  1.75 mins 
>>> Partitioning input file in 10 parts of size 96891 
>>> Terms integrity checked OK 
>>> Creating term frequencies dicts. 
    Processing list 1 with 96891 terms. 
    Processing list 2 with 97217 terms. 
    Processing list 3 with 99422 terms. 
    Processing list 4 with 96525 terms. 
    Processing list 5 with 96590 terms. 
    Processing list 6 with 101407 terms. 
    Processing list 7 with 95459 terms. 
    Processing list 8 with 95157 terms. 
    Processing list 9 with 95121 terms. 
    Processing list 10 with 95121 terms. 
>>> Clearing memory and unlisting dicts. 
>>> End of Job. Job time was: 1.29 mins 
>>> Replacing unfrequent terms as unknown. 
>>> Creating hash table. 
>>> End of script --createVocab--. 

> ngramLst <- Ngram.tf(x = trData_n, n = 2, fun = skip.ngram, 
+                      threshold = ngram_THRESH, 
+                      chunkSize = CH .... [TRUNCATED] 
>>> Creating 2-grams 
    5275741 terms generated. 
>>> Estimating time for main process. 
    Estimated time for main process:  1.71 hours 
>>> Partitioning input file in 10 parts of size 527574 
>>> Terms integrity checked OK 
>>> Creating term frequencies dicts. 
    Processing list 1 with 527575 terms. 
    Processing list 2 with 527575 terms. 
    Processing list 3 with 527574 terms. 
    Processing list 4 with 527573 terms. 
    Processing list 5 with 527925 terms. 
    Processing list 6 with 527540 terms. 
    Processing list 7 with 528132 terms. 
    Processing list 8 with 527282 terms. 
    Processing list 9 with 527282 terms. 
    Processing list 10 with 527283 terms. 
>>> Clearing memory and unlisting dicts. 
>>> End of Job. Job time was: 1.09 hours 

> #
> n2s <- ngramLst[[1]]

> n2sp <- ngramLst[[2]]

> T2s <- length(n2s)        # Number of Types

> N2s<- sum(n2s)            # Number of tokens

> n2s_df <- ngram.DF(n2s) 

> # End of script 